# 概述
aispace 25.0.0.1 融合部署（fke+aispace 融合部署方案）

# 安装宿主机操作系统
FusionOS 23.2.1，如 **FusionOS-23_23.2.1_everything_x86_64.iso**；

系统盘要求的介质类型/存储空间，**SSD / 480G及以上**；

语言要求为**英文**；

## 控制平面/计算 节点系统盘分区
系统分区严格按照 fke+aispace 融合部署方案，要求LVM类型分区，分控制平面/计算节点的系统盘分区，对于单节点需要均满足控制平面/计算节点的系统盘分区要求；

分区 /boot/efi 设置有要求，参考 **安装过程中的注意事项**；

控制平面节点的系统盘分区
| 分区                 | 大小    |
| -------------------- | ------- |
| /                    | 20GiB   |
| /root/bak            | 20GiB   |
| /var/lib/etcd        | 20GiB   |
| /var/log             | 36GiB   |
| /var/fusiononecenter | 80GiB   |
| /home                | 60GiB   |
| /boot                | 1024MiB |
| /boot/efi            | 600MiB  |

工作节点的系统盘分区
| 分区                 | 大小    |
| -------------------- | ------- |
| /                    | 20GiB   |
| /root/bak            | 20GiB   |
| /var/log             | 36GiB   |
| /home                | 60GiB   |
| /boot                | 1024MiB |
| /boot/efi            | 600MiB  |

安装过程中的注意事项，
- 若没有设置 UEFI，则 /boot/efi 分区无需存在；
- 不要设置 swap 分区；
- 需要修改 home 分区 `Modify` 弹框中的 `Size policy` 为 `As large as possible`，设置 `Name` 为 `fusionos`； **<------ 这个很重要，不做的话有可能会导致后面"部署容器"的 fke+aispace 一体化安装失败**

## 配置网络
单个网口配置的，这部分通过 BMC KVM 方式安装OS时可通过图形化界面进行配置，
```shell
ip addr

vim /etc/sysconfig/network-scripts/ifcfg-xxxxxx
  BOOTPROTO=static
  ONBOOT=yes
  IPADDR=x.x.x.x
  PREFIX=x.x.x.x / x
  GATEWAY=x.x.x.x
  DNS1=x.x.x.x

systemctl restart NetworkManager

ip addr
```
如果是组bond口的，参考产品文档；

# 宿主机操作系统设置
OS 安装完毕之后，进行一些基础设置， 

## 关闭 selinux
```shell
setenforce 0

vi /etc/selinux/config

```
修改参数项 `SELINUX` 为 `disabled` ，修改需要重启生效；

## 关闭防火墙
```shell
systemctl disable firewalld --now

systemctl status firewalld

```

## 配置最大文件打开数
配置 max_user_instances
```shell
echo fs.inotify.max_user_instances=8192| tee -a /etc/sysctl.conf && sudo sysctl -p
```

配置系统级资源限制
```shell
echo "root soft nofile 204800
  root hard nofile 204800
  root soft nproc 204800
  root hard nproc 204800" >> /etc/security/limits.conf
```

配置用户级资源限制
```shell
echo DefaultLimitNOFILE=204800 >>  /etc/systemd/system.conf
echo DefaultLimitNOFILE=204800 >>  /etc/systemd/user.conf
```
执行 `reboot -f` 命令重启系统才能使配置生效

执行以下命令检测配置是否成功
```shell
ulimit -n 
  204800
```

查看用户级是否修改成功
```shell
su - root -c 'ulimit -aHS' -s '/bin/bash'
```
回显示例如下，
```shell
su - root -c 'ulimit -aHS' -s '/bin/bash'
  real-time non-blocking time  (microseconds, -R) unlimited
  core file size              (blocks, -c) 0
  data seg size               (kbytes, -d) unlimited
  scheduling priority                 (-e) 0
  file size                   (blocks, -f) unlimited
  pending signals                     (-i) 63499
  max locked memory           (kbytes, -l) 2046468
  max memory size             (kbytes, -m) unlimited
  open files                          (-n) 204800
  pipe size                (512 bytes, -p) 8
  POSIX message queues         (bytes, -q) 819200
  real-time priority                  (-r) 0
  stack size                  (kbytes, -s) 8192
  cpu time                   (seconds, -t) unlimited
  max user processes                  (-u) 204800
  virtual memory              (kbytes, -v) unlimited
  file locks                          (-x) unlimited
```

## 配置时间
检查时区时间，设置时区
```shell
timedatectl
timedatectl set-timezone Asia/Shanghai
```
手动设置系统时间
```shell
date -s "xxxx-xx-xx xx:xx:xx"
```
将系统时间同步到硬件时钟
```shell
hwclock --systohc
```

# 安装 NV 显卡驱动
禁用nouveau驱动，执行以下命令将 nouveau 驱动加入黑名单，
```shell
echo "blacklist vga16fb
  blacklist nouveau
  blacklist rivafb
  blacklist rivatv
  blacklist nvidiafb" >> /etc/modprobe.d/blacklist.conf
```

执行 `dracut --force` 命令更新现有 initramfs 映像文件，
```shell
dracut --force
```

执行重启，使配置生效并检查，如果没有输出则证明禁用成功，
```shell
reboot -f
sudo lsmod | grep nouveau
```

安装 NV GPU 驱动，提前准备好驱动查询方法及驱动下载，

执行以下命令，查看服务器显卡信息，获取后4位PCI ID，示例为27b8，
```shell
sudo lspci | grep -i nvidia
```
回显示例如下，
```shell
[root@<hostname> ~]# sudo lspci | grep -i nvidia
b1:00.0 3D controller: NVIDIA Corporation Device 27b8 (rev a1)
b2:00.0 3D controller: NVIDIA Corporation Device 27b8 (rev a1)
[root@<hostname> ~]#
```
在 http://pci-ids.ucw.cz/mods/PC/10de/1eb8 上输入PCIID后，单击“Jump”，获取显卡型号，示例显卡型号为L4，

在 NVIDIA 官方网站下载对应版本的显卡驱动，

https://www.nvidia.cn/Download/index.aspx?lang=cn

执行以下命令检查是否已安装gcc、make，这里为 FusionOS 查询方法，
```shell
rpm -qa | grep gcc
make -v
```

如果已安装，将显示相关版本信息，如果未显示版本信息，则表示未安装，以下命令进行安装，这里为 FusionOS 安装方法，
```shell
yum install -y make gcc
```
如果有必要，安装 kernel-devel 和 kernel-headers ，
```shell
yum install kernel-devel-$(uname -r)
yum install kernel-headers-$(uname -r)
```

安装 NV GPU 驱动，安装方法如下，
```shell
sudo chmod a+x NVIDIA-Linux-x86_64-580.65.06.run
sudo ./NVIDIA-Linux-x86_64-580.65.06.run -no-x-check -no-nouveau-check -no-opengl-files
```

安装完毕之后，通过命令 nvidia-smi 查看，有类似回显表示安装成功，
```shell
nvidia-smi
  Thu Sep  4 15:20:26 2025
  +-----------------------------------------------------------------------------------------+
  | NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |
  +-----------------------------------------+------------------------+----------------------+
  | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
  | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
  |                                         |                        |               MIG M. |
  |=========================================+========================+======================|
  |   0  NVIDIA A40                     Off |   00000000:31:00.0 Off |                    0 |
  |  0%   38C    P8             31W /  300W |       0MiB /  46068MiB |      0%      Default |
  |                                         |                        |                  N/A |
  +-----------------------------------------+------------------------+----------------------+
  |   1  NVIDIA A40                     Off |   00000000:98:00.0 Off |                  Off |
  |  0%   36C    P8             31W /  300W |       0MiB /  49140MiB |      0%      Default |
  |                                         |                        |                  N/A |
  +-----------------------------------------+------------------------+----------------------+
  
  +-----------------------------------------------------------------------------------------+
  | Processes:                                                                              |
  |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
  |        ID   ID                                                               Usage      |
  |=========================================================================================|
  |  No running processes found                                                             |
  +-----------------------------------------------------------------------------------------+
```

执行以下命令，开启 NV GPU 的 Persistence Mode ，
```shell
nvidia-smi -pm 1
  Enabled Legacy persistence mode for GPU 00000000:31:00.0.
  Enabled Legacy persistence mode for GPU 00000000:98:00.0.
  All done.
```

安装完显卡驱动后，在官网下载并上传 nvidia-container-runtime 关联的安装包，安装命令以及安装完之后的检查方法，以 FusionOS 为例，
- libnvidia-container1 安装包
- libnvidia-container-tools 安装包
- nvidia-container-toolkit 安装包
- nvidia-container-toolkit-base 安装包

```shell
  -rw-r--r-- 1 root root   1061180 Aug 19 15:53 libnvidia-container1-1.13.1-1.x86_64.rpm
  -rw-r--r-- 1 root root     57320 Aug 19 15:53 libnvidia-container-tools-1.13.3-1.x86_64.rpm
  -rw-r--r-- 1 root root    935524 Aug 19 15:53 nvidia-container-toolkit-1.13.3-1.x86_64.rpm
  -rw-r--r-- 1 root root   3271024 Aug 19 15:53 nvidia-container-toolkit-base-1.13.3-1.x86_64.rpm
  -rwxr-xr-x 1 root root 393832804 Aug 19 11:41 NVIDIA-Linux-x86_64-580.65.06.run

rpm -Uvh --force --nodeps *.rpm

ll /dev/ |grep nvidia
  crw-rw-rw-  1 root root    195,   0 Sep  3 11:53 nvidia0
  crw-rw-rw-  1 root root    195,   1 Sep  3 11:53 nvidia1
  drwxr-xr-x  2 root root          80 Sep  3 11:53 nvidia-caps
  crw-rw-rw-  1 root root    195, 255 Sep  3 11:53 nvidiactl
  crw-rw-rw-  1 root root    195, 254 Sep  3 11:57 nvidia-modeset
  crw-rw-rw-  1 root root    510,   0 Sep  3 11:53 nvidia-uvm
  crw-rw-rw-  1 root root    510,   1 Sep  3 11:53 nvidia-uvm-tools
```

# 部署容器

## 安装 FKE 初始配置工具
严格执行以下命令，
```shell
mkdir /home/fke/installer
mkdir /home/fke/management
cp /the/path/of/FusionOneKubernetesEngine_BM_23.3.1_x86_64.zip /home/fke/management/
cp /the/path/of/FusionOneKubernetesEngine_BM_23.3.1_x86_64_Configure.zip /home/fke/installer/
cd /home/fke/installer/
unzip FusionOneKubernetesEngine_BM_23.3.1_x86_64_Configure.zip -d ./
sh /home/fke/installer/appctl.sh install
systemctl status configure-wizard
  Activce: active (running)
```

## 上传插件包
安装 XPU Engine 和 AI Space Prime 时必须执行，
```shell
mkdir -p /home/fke/charts/xpuengine
cp /the/path/of/FusionOneKubernetesEngine_XpuEngine_23.3.1_x86_64.zip /home/fke/charts/xpuengine/
mkdir -p /home/fke/charts/aispace
cp /the/path/of/AISpacePrime_25.0.0.1_fke_x86_64.zip  /home/fke/charts/aispace/
cp /the/path/of/AISpaceWings_25.0.0.1_x86_64.tgz  /home/fke/charts/aispace/
```

## 初始配置向导
登录 FKE 初始配置向导界面，单击“开始配置”，弹出许可协议对话框，点“同意”，单击“下一步”，进入配置集群界面，
- 节点信息-> root 密码，节点OS宿主机 root 密码，这里多节点要求一致；
- 节点类型/IP，如果是单节点不含计算节点，则控制平面节点即可，输入节点OS业务面IP地址；
- 集群网络配置，注意管理IP网络地址范围，与OS业务面IP地址/网段信息保持一致，其他默认；
- CSI配置，这里用 Local Storage CSI 举例；
- 镜像仓库配置，设置 harbor 的密码，以及 harbor 的本地目录；

然后点击校验

- 在线 yum 安装 OS 所需依赖
在配置 Local Storage CSI 之前，需要安装一些 OS 所需依赖，
```shell
yum install -y nfs nfs-utils rpcbind

systemctl enable nfs --now
systemctl enable rpcbind --now

systemctl status nfs
systemctl status rpcbind
```

- 挂载 OS 镜像作为 yum 源
附，yum源需单独下载对应节点的OS镜像进行配置，
挂载 OS iso 镜像，在 iBMC 界面挂载镜像，

创建临时挂载目录，
```shell
mkdir -p /mnt/cdrom
```

执行以下命令挂载镜像，
```shell
mount /dev/sr0 /mnt/cdrom
```

配置 yum 源
```shell
cp /etc/yum.repos.d /etc/yum.repos.d.bak
mkdir -p /etc/yum.repos.d
vim /etc/yum.repos.d/FusionOS.repo
  [OS]
  name=OS
  baseurl=file:///mnt/cdrom
  enabled=1
  gpgcheck=1
  gpgkey=file:///mnt/cdrom/RPM-GPG-KEY-FusionOS
```

- 将 OS iso 文件拷入某路径下，作为本地 yum 源
假设，FusionOS-23_23.2.1_everything_x86_64.iso 放置在 /root/iso/ 下，
```shell
mkdir -p /mnt/fusionos23
mount -o loop,ro /root/iso/FusionOS-23_23.2.1_everything_x86_64.iso /mnt/fusionos23

mount | grep fusionos23

mkdir -p /etc/yum.repos.d/bak
mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/bak/

cat > /etc/yum.repos.d/FusionOS-23-local.repo <<'EOF'
[FusionOS-23-Local]
name=FusionOS 23.2.1 Everything (Local)
baseurl=file:///mnt/fusionos23
enabled=1
gpgcheck=0
skip_if_unavailable=1
EOF

yum clean all
yum makecache
yum repolist            # 应能看到 FusionOS-23-Local 行

yum -y install tree

开机自动挂载（可选）
在 /etc/fstab 尾部追加一行，
/root/iso/FusionOS-23_23.2.1_everything_x86_64.iso /mnt/fusionos23 iso9660 loop,ro 0 0
保存后执行

mount -a
```

### 镜像仓库本地存储配置 Local Storage CSI
必须要求使用空间最低配置为600GB，可三块 ssd 组 RAID5 阵列；
```shell
fdisk /dev/sda
  n          # 新建分区
  p          # 选择主分区（默认）
  1          # 分区号设为 1（即 sdb1）
             # 起始扇区直接回车（默认从磁盘开头）
  +10G       # 分区大小（示例：10GB，可按需修改）

lsblk
pvcreate /dev/sda1
vgcreate harbordata /dev/sda1
lvcreate -n harbordata -l 100%FREE harbordata
mkfs.ext4 /dev/harbordata/harbordata 
mkdir -p /harbor_data
mount /dev/harbordata/harbordata /harbor_data/
vim /etc/fstab
  /dev/harbordata/harbordata /harbor_data/ ext4 defaults 1 2

mount -a
```

### AI Space Prime 插件的 NFS 服务配置
这里，环境使用 在物理机容器节点上配置 NFS 服务 的方式，
```shell
fdisk /dev/sda
  n          # 新建分区
  p          # 选择主分区（默认）
  2          # 分区号设为 1（即 sdb1）
             # 起始扇区直接回车（默认从磁盘开头）
  +10G       # 分区大小（示例：10GB，可按需修改）

lsblk
pvcreate /dev/sda2
vgcreate nfsdata /dev/sda2
lvcreate -n nfsdata -l 100%FREE nfsdata
mkfs.ext4 /dev/mapper/nfsdata-nfsdata 
mkdir -p /nfs_data
mount /dev/mapper/nfsdata-nfsdata /nfs_data/
vim /etc/fstab
  /dev/mapper/nfsdata-nfsdata /nfs_data/ ext4 defaults 1 2

mount -a

chmod +666 /nfs_data
mkdir -p /etc/ganesha
cd /etc/ganesha/
vim inner-aimodel-vfs.conf
  EXPORT
  {
      # Export Id (mandatory, each EXPORT must have a unique Export_Id)
      Export_Id = 772;
      # Exported path (mandatory)
      #Path = /var/atlasdirector/shared;
      Path = /nfs_data;     ## 修改为待挂载目录
      # Pseudo Path (required for NFS v4)
      Pseudo = /nfs_data;   ## 修改为待挂载目录
      # Required for access (default is None)
      # Could use CLIENT blocks instead
      # Exporting FSAL
      FSAL {
          Name = VFS;
      }
      Disable_ACL = FALSE;  # To enable/disable ACL
      Protocols = "3", "4"; # NFS protocols supported
      Transports = "UDP", "TCP"; # Transport protocols supported
      SecType = "sys";      # Security flavors supported
      Squash = no_root_squash;
      CLIENT
      {
          Clients = *;      # 允许所有其他客户端访问
          Access_Type = RW; # 为其他客户端设置只读权限
      }
  }
```

## 检查环境
```shell
kubectl get pods -n ai-system
```









